{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from transformers.trainer_utils import set_seed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers.generation import GenerationConfig\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import jsonlines\n",
    "import textwrap\n",
    "\n",
    "\n",
    "class args:\n",
    "    checkpoint_path = '/gemini/code/lamma3_eval/lamma3_model/8B_instruct'\n",
    "    eval_data_path = '/gemini/code/lamma3_eval/eval_data/humaneval'\n",
    "    save_result_dir = \"/gemini/code/lamma3_eval/eval_result/humaneval_chat\"\n",
    "    # choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    debug = False\n",
    "    overwrite = False\n",
    "    batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"openai_humaneval\")\n",
    "# dataset.save_to_disk(args.eval_data_path)\n",
    "\n",
    "dataset = load_from_disk(args.eval_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HumanEval/0', 'HumanEval/1', 'HumanEval/2', 'HumanEval/3', 'HumanEval/4', 'HumanEval/5', 'HumanEval/6', 'HumanEval/7', 'HumanEval/8', 'HumanEval/9', 'HumanEval/10', 'HumanEval/11', 'HumanEval/12', 'HumanEval/13', 'HumanEval/14', 'HumanEval/15', 'HumanEval/16', 'HumanEval/17', 'HumanEval/18', 'HumanEval/19', 'HumanEval/20', 'HumanEval/21', 'HumanEval/22', 'HumanEval/23', 'HumanEval/24', 'HumanEval/25', 'HumanEval/26', 'HumanEval/27', 'HumanEval/28', 'HumanEval/29', 'HumanEval/30', 'HumanEval/31', 'HumanEval/32', 'HumanEval/33', 'HumanEval/34', 'HumanEval/35', 'HumanEval/36', 'HumanEval/37', 'HumanEval/38', 'HumanEval/39', 'HumanEval/40', 'HumanEval/41', 'HumanEval/42', 'HumanEval/43', 'HumanEval/44', 'HumanEval/45', 'HumanEval/46', 'HumanEval/47', 'HumanEval/48', 'HumanEval/49', 'HumanEval/50', 'HumanEval/51', 'HumanEval/52', 'HumanEval/53', 'HumanEval/54', 'HumanEval/55', 'HumanEval/56', 'HumanEval/57', 'HumanEval/58', 'HumanEval/59', 'HumanEval/60', 'HumanEval/61', 'HumanEval/62', 'HumanEval/63', 'HumanEval/64', 'HumanEval/65', 'HumanEval/66', 'HumanEval/67', 'HumanEval/68', 'HumanEval/69', 'HumanEval/70', 'HumanEval/71', 'HumanEval/72', 'HumanEval/73', 'HumanEval/74', 'HumanEval/75', 'HumanEval/76', 'HumanEval/77', 'HumanEval/78', 'HumanEval/79', 'HumanEval/80', 'HumanEval/81', 'HumanEval/82', 'HumanEval/83', 'HumanEval/84', 'HumanEval/85', 'HumanEval/86', 'HumanEval/87', 'HumanEval/88', 'HumanEval/89', 'HumanEval/90', 'HumanEval/91', 'HumanEval/92', 'HumanEval/93', 'HumanEval/94', 'HumanEval/95', 'HumanEval/96', 'HumanEval/97', 'HumanEval/98', 'HumanEval/99', 'HumanEval/100', 'HumanEval/101', 'HumanEval/102', 'HumanEval/103', 'HumanEval/104', 'HumanEval/105', 'HumanEval/106', 'HumanEval/107', 'HumanEval/108', 'HumanEval/109', 'HumanEval/110', 'HumanEval/111', 'HumanEval/112', 'HumanEval/113', 'HumanEval/114', 'HumanEval/115', 'HumanEval/116', 'HumanEval/117', 'HumanEval/118', 'HumanEval/119', 'HumanEval/120', 'HumanEval/121', 'HumanEval/122', 'HumanEval/123', 'HumanEval/124', 'HumanEval/125', 'HumanEval/126', 'HumanEval/127', 'HumanEval/128', 'HumanEval/129', 'HumanEval/130', 'HumanEval/131', 'HumanEval/132', 'HumanEval/133', 'HumanEval/134', 'HumanEval/135', 'HumanEval/136', 'HumanEval/137', 'HumanEval/138', 'HumanEval/139', 'HumanEval/140', 'HumanEval/141', 'HumanEval/142', 'HumanEval/143', 'HumanEval/144', 'HumanEval/145', 'HumanEval/146', 'HumanEval/147', 'HumanEval/148', 'HumanEval/149', 'HumanEval/150', 'HumanEval/151', 'HumanEval/152', 'HumanEval/153', 'HumanEval/154', 'HumanEval/155', 'HumanEval/156', 'HumanEval/157', 'HumanEval/158', 'HumanEval/159', 'HumanEval/160', 'HumanEval/161', 'HumanEval/162', 'HumanEval/163']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][\"task_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.checkpoint_path,\n",
    "        padding_side='left'\n",
    "    )\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.checkpoint_path,\n",
    "        device_map=\"auto\",\n",
    "        # quantization_config=quantization_config\n",
    "        # torch_dtype=torch.bfloat16\n",
    "    ).eval()\n",
    "    model.generation_config = GenerationConfig.from_pretrained(\n",
    "        args.checkpoint_path\n",
    "    )\n",
    "    # model.generation_config.do_sample = False  # use greedy decoding\n",
    "    # model.generation_config.repetition_penalty = 1.0  # disable repetition penalty\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.22s/it]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_models_tokenizer()\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process(func, *args):\n",
    "    '''\n",
    "    args 负责接受 一个或多个 batch\n",
    "    '''\n",
    "    # print(f'args len: {len(args)}')\n",
    "    if len(args) > 1:\n",
    "        elem_len = len(args[0])\n",
    "        assert all(len(elem) == elem_len for elem in args), \"各参数长度不同\"\n",
    "\n",
    "    text_ls = []\n",
    "    for sample in zip(*args):\n",
    "        text_ls.append(func(*sample))\n",
    "    return text_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_output_item(text, raw_txt_len):  # 可能需要改进，可以把raw_text长度传进来，直接截断，然后提取第一个回答。这里相当 取了最后一个回答\n",
    "    pad = tokenizer.pad_token\n",
    "    output = text\n",
    "    \n",
    "    # 先去掉填充\n",
    "    output = output.replace(pad, \"\").strip()\n",
    "    # 再截取\n",
    "    output = output[raw_txt_len:]\n",
    "\n",
    "    # 最后去掉停止token\n",
    "    stop_words = [\"<|end_of_text|>\", \"<|eot_id|>\"]\n",
    "    for sw in stop_words:\n",
    "        output = output.replace(sw, \"\").strip()\n",
    "    \n",
    "    assert output != \"\", f\"输出为空\\n{text}\"\n",
    "    return output\n",
    "\n",
    "def clear_output(text, raw_txt_len):\n",
    "    return batch_process(clear_output_item, text, raw_txt_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, input_txt):\n",
    "    chat_template = [[{'content': t, 'role': 'user'}] for t in input_txt]\n",
    "    input_txt = tokenizer.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer(input_txt, padding=True, return_tensors=\"pt\",add_special_tokens=False).to(model.device)\n",
    "\n",
    "    raw_txt_len = [len(t) for t in input_txt]\n",
    "    # print(input_txt)\n",
    "    # print(input_ids['input_ids'])\n",
    "    # print(input_ids['attention_mask'])\n",
    "\n",
    "    outputs_id = model.generate(**input_ids, max_new_tokens = 256, eos_token_id = 128009, pad_token_id = tokenizer.pad_token_id)\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(outputs_id, skip_special_tokens=False)\n",
    "    answer = clear_output(outputs, raw_txt_len)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_item(text, entry_point):\n",
    "    # 正则表达式匹配代码块\n",
    "    code_block_pattern = re.compile(\n",
    "        rf\"```(?:[Pp]ython\\n)?.*?def\\s+{entry_point}.*?:\\n(.*?)\\n```\", re.DOTALL\n",
    "    )\n",
    "    code_block = code_block_pattern.search(text)\n",
    "\n",
    "    if code_block is None:\n",
    "        code_block_pattern = re.compile(\n",
    "            rf\"def\\s+{entry_point}.*?:\\n(.*?)(?:\\n(?!\\n*(?:  |\\t))|$)\", re.DOTALL\n",
    "        )\n",
    "        code_block = code_block_pattern.search(text)\n",
    "    if code_block is None:\n",
    "        code_block_pattern = re.compile(\n",
    "            r\"def.*?:\\n(.*?)(?:\\n(?!\\n*(?:  |\\t))|$)\", re.DOTALL\n",
    "        )\n",
    "        code_block = code_block_pattern.search(text)\n",
    "\n",
    "    if code_block is not None:\n",
    "        return code_block.group(1)\n",
    "\n",
    "    # if no code block is found, assume the LM is simply filling the code\n",
    "    return textwrap.indent(text, \" \" * 4)\n",
    "\n",
    "\n",
    "def extract_code(text, entry_point):\n",
    "    return batch_process(extract_code_item, text, entry_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(model, tokenizer, question, entry_point):\n",
    "    response = generate_answer(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        question\n",
    "    )\n",
    "    # print(question)\n",
    "    # print(response)\n",
    "    answer = extract_code(response, entry_point)\n",
    "    return answer, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_item(question):\n",
    "    signature = re.search(\n",
    "        rf\"def\\s+({question['entry_point']}.*?):\\s*\\n\", question[\"prompt\"]\n",
    "    ).group(1)\n",
    "    # print(signature)\n",
    "    description = \"\\n\".join(\n",
    "        [\n",
    "            line.strip()\n",
    "            for line in re.search(\n",
    "                rf\"(?:\\\"\\\"\\\"|''')(.*?)(?:\\\"\\\"\\\"|''')\", question[\"prompt\"], re.DOTALL\n",
    "            )\n",
    "            .group(1)\n",
    "            .split(\"\\n\")\n",
    "        ]\n",
    "    )\n",
    "    # print(description)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Write a Python function `{signature}` to solve the following problem, just give the code:\\n\"\n",
    "        f\"{description}\\n\"\n",
    "        f\"{question['prompt']}\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def preprocess(question):\n",
    "    return batch_process(preprocess_item, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    result_path = os.path.join(args.save_result_dir, f\"result.jsonl\")\n",
    "    if not args.overwrite and os.path.exists(result_path):\n",
    "        print(f\"{result_path} existed, skip!\")\n",
    "\n",
    "    else:\n",
    "        test = dataset[\"test\"]\n",
    "        os.makedirs(args.save_result_dir, exist_ok=True)\n",
    "        f_output = jsonlines.Writer(open(result_path, \"w\", encoding=\"utf-8\"))\n",
    "\n",
    "        with f_output as output:\n",
    "            for i in tqdm(range(0, len(test), args.batch_size)):\n",
    "                batch = test.select(range(i, min(i+args.batch_size, len(test))))\n",
    "                prompt = preprocess(batch)\n",
    "                task_id = batch[\"task_id\"]\n",
    "\n",
    "                answer, response = generate_sample(\n",
    "                    model, tokenizer, prompt, batch[\"entry_point\"]\n",
    "                )\n",
    "                for i in range(len(batch)):\n",
    "                    gen = {\"task_id\": task_id[i], \"completion\": answer[i], \"response\": response[i]}\n",
    "                    output.write(gen)\n",
    "        f_output.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [08:46<00:00, 47.85s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
