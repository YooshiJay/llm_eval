{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers.trainer_utils import set_seed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers.generation import GenerationConfig\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "class args:\n",
    "    checkpoint_path = '/gemini/code/lamma3_eval/lamma3_model/8B'\n",
    "    eval_data_path = '/gemini/code/lamma3_eval/eval_data/mmlu'\n",
    "    save_result_dir = \"/gemini/code/lamma3_eval/eval_result/mmlu\"\n",
    "    choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    debug = False\n",
    "    overwrite = False\n",
    "    batch_size = 4\n",
    "    max_seq_len = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME_MAPPING = {\n",
    "    \"stem\": [\n",
    "        \"abstract_algebra\",\n",
    "        \"anatomy\",\n",
    "        \"astronomy\",\n",
    "        \"college_biology\",\n",
    "        \"college_chemistry\",\n",
    "        \"college_computer_science\",\n",
    "        \"college_mathematics\",\n",
    "        \"college_physics\",\n",
    "        \"computer_security\",\n",
    "        \"conceptual_physics\",\n",
    "        \"electrical_engineering\",\n",
    "        \"elementary_mathematics\",\n",
    "        \"high_school_biology\",\n",
    "        \"high_school_chemistry\",\n",
    "        \"high_school_computer_science\",\n",
    "        \"high_school_mathematics\",\n",
    "        \"high_school_physics\",\n",
    "        \"high_school_statistics\",\n",
    "        \"machine_learning\",\n",
    "    ],\n",
    "    \"Humanities\": [\n",
    "        \"formal_logic\",\n",
    "        \"high_school_european_history\",\n",
    "        \"high_school_us_history\",\n",
    "        \"high_school_world_history\",\n",
    "        \"international_law\",\n",
    "        \"jurisprudence\",\n",
    "        \"logical_fallacies\",\n",
    "        \"moral_disputes\",\n",
    "        \"moral_scenarios\",\n",
    "        \"philosophy\",\n",
    "        \"prehistory\",\n",
    "        \"professional_law\",\n",
    "        \"world_religions\",\n",
    "    ],\n",
    "    \"other\": [\n",
    "        \"business_ethics\",\n",
    "        \"college_medicine\",\n",
    "        \"human_aging\",\n",
    "        \"management\",\n",
    "        \"marketing\",\n",
    "        \"medical_genetics\",\n",
    "        \"miscellaneous\",\n",
    "        \"nutrition\",\n",
    "        \"professional_accounting\",\n",
    "        \"professional_medicine\",\n",
    "        \"virology\",\n",
    "        \"global_facts\",\n",
    "        \"clinical_knowledge\",\n",
    "    ],\n",
    "    \"social\": [\n",
    "        \"econometrics\",\n",
    "        \"high_school_geography\",\n",
    "        \"high_school_government_and_politics\",\n",
    "        \"high_school_macroeconomics\",\n",
    "        \"high_school_microeconomics\",\n",
    "        \"high_school_psychology\",\n",
    "        \"human_sexuality\",\n",
    "        \"professional_psychology\",\n",
    "        \"public_relations\",\n",
    "        \"security_studies\",\n",
    "        \"sociology\",\n",
    "        \"us_foreign_policy\",\n",
    "    ],\n",
    "}\n",
    "SUBJECTS = [v for vl in TASK_NAME_MAPPING.values() for v in vl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.checkpoint_path,\n",
    "        padding_side='left',\n",
    "        pad_token='<|reserved_special_token_0|>'\n",
    "    )\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.checkpoint_path,\n",
    "        device_map=\"auto\",\n",
    "        # quantization_config=quantization_config\n",
    "    ).eval()\n",
    "    model.generation_config = GenerationConfig.from_pretrained(\n",
    "        args.checkpoint_path\n",
    "    )\n",
    "    model.generation_config.do_sample = False  # use greedy decoding\n",
    "    model.generation_config.repetition_penalty = 1.0  # disable repetition penalty\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.65s/it]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_models_tokenizer()\n",
    "# dataset = load_from_disk(args.eval_data_path)\n",
    "# dev = dataset['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(line, include_answer=True):\n",
    "    example = \"Question: \" + line[\"question\"]\n",
    "    for i, choice in enumerate(args.choices):\n",
    "        example += f'\\n{choice}. {line[\"choices\"][i]}'\n",
    "\n",
    "    if include_answer:\n",
    "        example += \"\\nAnswer: \" + args.choices[line[\"answer\"]] + \"\\n\\n\"\n",
    "    else:\n",
    "        example += \"\\nAnswer:\"\n",
    "    return example\n",
    "\n",
    "\n",
    "def generate_few_shot_prompt(dev, subject_name):\n",
    "    def format_subject(subject):\n",
    "        l = subject.split(\"_\")\n",
    "        s = \"\"\n",
    "        for entry in l:\n",
    "            s += \" \" + entry\n",
    "        return s.strip()\n",
    "\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(format_subject(subject_name))\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        prompt += format_example(\n",
    "            dev[i],\n",
    "            include_answer=True,\n",
    "        )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def doc_to_text(doc, few_shot_prompt): # doc 是 dataset\n",
    "    return batch_process(lambda x: few_shot_prompt + format_example(x, include_answer=False), doc)\n",
    "\n",
    "\n",
    "def batch_process(func, *args):\n",
    "    '''\n",
    "    args 负责接受 一个或多个 batch\n",
    "    '''\n",
    "    \n",
    "    text_ls = []\n",
    "    # 只收到一个迭代元素时，zip会自动 将其的每个元素 单独包装成一个元组\n",
    "    # [1, 2, 3] -> [(1,), (2,), (3,)]\n",
    "    for sample in zip(*args): \n",
    "        text_ls.append(func(*sample))\n",
    "    return text_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(tokenizer, model, inputs: List[str]):\n",
    "    input_ids = tokenizer(inputs, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "    # print(input_ids[\"input_ids\"].shape[1])\n",
    "    cur_len = input_ids[\"input_ids\"].shape[1]\n",
    "    if cur_len > args.max_seq_len:\n",
    "        input_ids[\"input_ids\"] = input_ids[\"input_ids\"][:, cur_len - args.max_seq_len :]\n",
    "        input_ids[\"attention_mask\"] = input_ids[\"attention_mask\"][:, cur_len - args.max_seq_len :]\n",
    "\n",
    "    tokens = {\"input_ids\": input_ids}\n",
    "\n",
    "    outputs = model(**input_ids)[\"logits\"]\n",
    "    logits = outputs[:, -1, :]  # (batch, 每个子段 对下个token的预测, 词表size)  只要下个token的预测，取最后一个\n",
    "    # log_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return logits, {\"tokens\": tokens}\n",
    "\n",
    "\n",
    "def is_correct(pred, answer):\n",
    "    return batch_process(lambda x, y: x==y, pred, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather 表示根据 index “聚集” logits 对应位置的元素\n",
    "# 在 index 张量中，一个元素会处在某个位置，dim 表示：替换这个元素位置（会是张量维度大小）的第dim维度 -> 元素的值\n",
    "# 比如 dim = -1, index[0,0] = 1，含义为：结果的[0,0]位置 填 input[0,1]\n",
    "# input:\n",
    "# tensor([[ 0,  1,  2,  3,  4],\n",
    "#         [ 5,  6,  7,  8,  9],\n",
    "#         [10, 11, 12, 13, 14]])\n",
    "# index:\n",
    "# tensor([[1, 0],\n",
    "#         [0, 0],\n",
    "#         [1, 2]])\n",
    "# dim=1时:\n",
    "# tensor([[ 1,  0],\n",
    "#         [ 5,  5],\n",
    "#         [11, 12]])\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_subject(subject_name, dataset):\n",
    "    # torch.cuda.empty_cache()\n",
    "    test = dataset['test']\n",
    "    dev = dataset['dev']\n",
    "    question_ls = []\n",
    "    answer_ls = []\n",
    "    score = []\n",
    "    result = []\n",
    "\n",
    "    few_shot_prompt = generate_few_shot_prompt(dev, subject_name) # 5 shot\n",
    "    if args.debug:\n",
    "        print(f\"few_shot_prompt: {few_shot_prompt}\")\n",
    "    \n",
    "    choices_ids = torch.tensor(\n",
    "        tokenizer(\" A\")[\"input_ids\"][1:] + tokenizer(\" B\")[\"input_ids\"][1:] +\n",
    "        tokenizer(\" C\")[\"input_ids\"][1:] + tokenizer(\" D\")[\"input_ids\"][1:]\n",
    "    ).unsqueeze(0).to(model.device)\n",
    "\n",
    "    all_probs = {\"prob_A\": [], \"prob_B\": [], \"prob_C\": [], \"prob_D\": []}\n",
    "    for i in tqdm(range(0, len(test), args.batch_size)):\n",
    "        batch = test.select(range(i, min(i+args.batch_size, len(test))))\n",
    "        context = doc_to_text(batch, few_shot_prompt)\n",
    "        logits, input_info = get_logits(tokenizer, model, context)\n",
    "        \n",
    "        softval = logits.gather(dim=1, index=choices_ids.expand(logits.size(0), -1)).softmax(1)\n",
    "        if softval.dtype in {torch.bfloat16, torch.float16}:\n",
    "            softval = softval.to(dtype=torch.float32)\n",
    "        probs = softval.detach().cpu().numpy()\n",
    "        if args.debug:\n",
    "            print(probs)\n",
    "\n",
    "        for i in range(len(probs)):\n",
    "            for j, choice in enumerate(args.choices):\n",
    "                all_probs[f\"prob_{choice}\"].append(probs[i][j])\n",
    "        \n",
    "        pred = np.argmax(probs, axis=-1)\n",
    "        answer = batch['answer']\n",
    "        acc = is_correct(pred, answer)\n",
    "\n",
    "        if args.debug:\n",
    "            for i in range(len(batch)):\n",
    "                print(f'{batch[\"question\"][i]} \\npred: {pred[i]} \\nref: {answer[i]}\\n')\n",
    "            \n",
    "        question_ls.extend(context)\n",
    "        answer_ls.extend(answer)\n",
    "        result.extend(pred)\n",
    "        score.extend(acc)\n",
    "         \n",
    "\n",
    "    return question_ls, answer_ls, result, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    all_question = []\n",
    "    all_answer = []\n",
    "    all_result = []\n",
    "    all_score = []\n",
    "\n",
    "    # 看有无文件，有的话就不重复做了\n",
    "    result_path = os.path.join(args.save_result_dir, f\"result.csv\")\n",
    "    if not args.overwrite and os.path.exists(result_path):\n",
    "        print(f\"{result_path} existed, skip!\")\n",
    "        for (_, resultrow) in pd.read_csv(result_path).iterrows():\n",
    "            # pred = extract_answer(resultrow['model_response'], datarow)\n",
    "            acc = resultrow[\"ACC\"]\n",
    "            all_score.append(acc)\n",
    "\n",
    "    else:\n",
    "        for subject_name in tqdm(SUBJECTS):\n",
    "            # print(subject_name)\n",
    "            dev_file_path = os.path.join(\n",
    "                args.eval_data_path, subject_name, \"dev-00000-of-00001.parquet\"\n",
    "            )\n",
    "            test_file_path = os.path.join(\n",
    "                args.eval_data_path, subject_name, \"test-00000-of-00001.parquet\"\n",
    "            )\n",
    "\n",
    "            dataset = load_dataset(\"parquet\", data_files={'dev': dev_file_path, 'test': test_file_path})\n",
    "            \n",
    "            question_ls, answer_ls, result, score = eval_subject(subject_name, dataset)\n",
    "            all_question.extend(question_ls)\n",
    "            all_answer.extend(answer_ls)\n",
    "            all_result.extend(result)\n",
    "            all_score.extend(score)\n",
    "\n",
    "\n",
    "        # 存入文件\n",
    "        output_df = pd.DataFrame(\n",
    "            {\"model_question\": all_question,\n",
    "            \"standard_answer\": all_answer,\n",
    "            \"model_response\": all_result, \n",
    "            \"ACC\": all_score}\n",
    "        )\n",
    "        os.makedirs(args.save_result_dir, exist_ok=True)\n",
    "        result_path = os.path.join(args.save_result_dir, f\"result.csv\")\n",
    "        output_df.to_csv(\n",
    "            result_path,\n",
    "            encoding=\"utf-8\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    print(\"AVERAGE ACC:%.2f \" % (sum(all_score) / len(all_score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/lamma3_eval/eval_result/mmlu/result.csv existed, skip!\n",
      "AVERAGE ACC:65.47 \n"
     ]
    }
   ],
   "source": [
    "# !export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:8192\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # high_school_european_history 中 question + few_shot 达到 3000 token，需要限制token数量。\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tokenizer(\"hi help help A \",return_tensors='pt')\n",
    "# tt = model(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   6151,   1520,   1520,    362,    220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[1520, 1520,  362,  220]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# a\n",
    "# print(a)\n",
    "# cur_len = a[\"input_ids\"].shape[1]\n",
    "# a[\"input_ids\"] = a[\"input_ids\"][:, cur_len - 4  :]\n",
    "# a[\"attention_mask\"] = a[\"attention_mask\"][:, cur_len - 4 :]\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' me'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l = tt['logits'][:,2,:]\n",
    "# lp = torch.nn.functional.softmax(l, dim=-1)\n",
    "# tokenizer.decode(torch.tensor([np.argmax(lp.detach().cpu().numpy())]))\n",
    "\n",
    "# torch.tensor([[np.argmax(lp.detach().cpu().numpy())]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  14924,     25,  ...,    430,    682,   2380],\n",
       "        [128000,  14924,     25,  ...,    220,     16,     14]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# prompt = '''Question: In 2004, there were 60 kids at a cookout. In 2005, half the number of kids came to the cookout as compared to 2004. In 2006, 2/3 as many kids came to the cookout as in 2005. How many kids came to the cookout in 2006?\n",
    "# Let's think step by step\n",
    "# In 2005, 60/2=30 kids came to the cookout.\n",
    "# In 2006, 30/3*2=20 kids came to the cookout.\n",
    "# The answer is 20'''\n",
    "\n",
    "# q1 = r\"Zilla spent 7% of her monthly earnings on rent, half of it on her other monthly expenses, and put the rest in her savings. If she spent $133 on her rent, how much does she deposit into her savings account in a month?\"\n",
    "# q2 = r\"If Buzz bought a pizza with 78 slices at a restaurant and then decided to share it with the waiter in the ratio of 5:8, with Buzz's ratio being 5, what's twenty less the number of slices of pizza that the waiter ate?\"\n",
    "\n",
    "# a1 = prompt + \"\\nQuestion: \" \\\n",
    "#         + q1 \\\n",
    "#         + \"\\nLet's think step by step\\n\"\n",
    "\n",
    "# a2 = prompt + \"\\nQuestion: \" \\\n",
    "#         + q2 \\\n",
    "#         + \"\\nLet's think step by step\\n\"\n",
    "\n",
    "# aa = torch.tensor(tokenizer([a1, a2], padding='longest')['input_ids']).to(model.device)\n",
    "# # # tokenizer.encode([\" A\",'B324'], padding='longest')\n",
    "# mask = aa.ne(tokenizer.pad_token_id)\n",
    "# # print(aa)\n",
    "# tt = model.generate(aa, attention_mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sss = tokenizer.decode(tt[1], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
